# Adam Optimizer for Automatic Parameter Tuning

## Overview

The market maker now uses the **Adam optimizer** (Adaptive Moment Estimation) for **fully autonomous** parameter tuning. This is a self-learning system that continuously adapts to market conditions without manual intervention.

## ü§ñ Autonomous Operation

**This is a self-tuning agent.** The `tuning_params.json` file serves only two purposes:

1. **Initial parameters** when the bot starts up
2. **Persistence** of Adam's learned parameters across restarts

### How to Override Parameters

If you want to manually change parameters:

```bash
# 1. Stop the bot
Ctrl+C

# 2. Edit tuning_params.json with your desired starting parameters
nano tuning_params.json

# 3. Restart the bot
RUST_LOG=info cargo run --bin market_maker_v2
```

Adam will immediately begin learning from your new starting point.

### What Changed

‚ùå **REMOVED**: Hot-reload functionality (checking file every 10 seconds)  
‚úÖ **NEW**: Adam optimizer has full authority over parameter tuning  
‚úÖ **NEW**: Parameters only loaded once at startup  
‚úÖ **NEW**: Adam continuously saves learned parameters to JSON

**Why this is better:**
- No conflicting updates between file changes and Adam's learning
- Cleaner separation of concerns (human sets starting point, Adam optimizes)
- Adam's learning trajectory is never interrupted
- Simpler, more predictable behavior

## What is Adam?

Adam is an adaptive learning rate optimization algorithm introduced by Kingma & Ba (2015). It combines the benefits of:
- **AdaGrad**: Adapts learning rates based on parameter-specific gradient history
- **RMSProp**: Uses exponential moving averages to prevent learning rate decay
- **Momentum**: Smooths optimization by accumulating gradients over time

## Key Advantages Over Vanilla SGD

### 1. **Automatic Learning Rate Adaptation**
- No need to manually tune the learning rate (`Œ∑`)
- Each parameter gets its own adaptive learning rate
- Learning rates automatically adjust based on gradient magnitude and variance

### 2. **Robust to Market Volatility**
- Momentum smoothing prevents overreaction to single noisy gradients
- Second moment estimation accounts for gradient variance
- More stable convergence in volatile market conditions

### 3. **Faster Convergence**
- Typically converges 2-5x faster than vanilla SGD
- Can escape local minima more effectively
- Handles sparse gradients well (when some parameters don't need adjustment)

### 4. **Less Sensitive to Initialization**
- Default `Œ± = 0.001` works well in most cases
- Bias correction ensures good performance from the start
- Momentum parameters (`Œ≤‚ÇÅ = 0.9`, `Œ≤‚ÇÇ = 0.999`) are well-tuned defaults

## How It Works

### Loss Function: Control Gap (Not Value Gap!)

**Critical Design Decision**: The loss function is the **squared difference in quote offsets**, NOT the difference in expected values:

```
L = (Œ¥^b_optimal - Œ¥^b_heuristic)¬≤ + (Œ¥^a_optimal - Œ¥^a_heuristic)¬≤
```

Where:
- `Œ¥^b_optimal` = Optimal bid offset from grid search (in bps)
- `Œ¥^b_heuristic` = Heuristic bid offset from `apply_state_adjustments()` (in bps)
- `Œ¥^a_optimal` = Optimal ask offset from grid search (in bps)  
- `Œ¥^a_heuristic` = Heuristic ask offset from `apply_state_adjustments()` (in bps)

**Why this is better than value gap:**

1. **Stable gradients**: Quote offsets are in bps (typically 5-50 bps), producing gradients in a reasonable range
2. **Interpretable**: Loss of 4.0 means quotes differ by 2 bps RMS (root mean square)
3. **Direct optimization**: We want matching quotes, not matching abstract "values"
4. **Numerically well-behaved**: No issues with large or near-zero denominators

**Example**:
```
Optimal:    bid_offset = 5.0 bps,  ask_offset = 5.0 bps
Heuristic:  bid_offset = 7.5 bps,  ask_offset = 4.0 bps

Loss = (5.0 - 7.5)¬≤ + (5.0 - 4.0)¬≤
     = (-2.5)¬≤ + (1.0)¬≤
     = 6.25 + 1.0
     = 7.25 bps¬≤
```

This is a manageable number that produces gradients Adam can work with effectively.

### Update Rule

For each parameter `Œ∏·µ¢` at time step `t`:

```
1. Compute gradient: g‚Çú = ‚àáL(Œ∏‚Çú)

2. Update first moment (mean):
   m‚Çú = Œ≤‚ÇÅ ¬∑ m‚Çú‚Çã‚ÇÅ + (1 - Œ≤‚ÇÅ) ¬∑ g‚Çú

3. Update second moment (variance):
   v‚Çú = Œ≤‚ÇÇ ¬∑ v‚Çú‚Çã‚ÇÅ + (1 - Œ≤‚ÇÇ) ¬∑ g‚Çú¬≤

4. Bias correction:
   mÃÇ‚Çú = m‚Çú / (1 - Œ≤‚ÇÅ·µó)
   vÃÇ‚Çú = v‚Çú / (1 - Œ≤‚ÇÇ·µó)

5. Parameter update:
   Œ∏‚Çú‚Çä‚ÇÅ = Œ∏‚Çú - Œ± ¬∑ mÃÇ‚Çú / (‚àövÃÇ‚Çú + Œµ)
```

Where:
- `Œ± = 0.001` - Base learning rate (conservative default)
- `Œ≤‚ÇÅ = 0.9` - First moment decay (momentum)
- `Œ≤‚ÇÇ = 0.999` - Second moment decay (variance)
- `Œµ = 1e-8` - Numerical stability constant

## Implementation Details

### AdamOptimizerState Struct

```rust
pub struct AdamOptimizerState {
    pub m: Vec<f64>,        // First moment estimates (7 params)
    pub v: Vec<f64>,        // Second moment estimates (7 params)
    pub t: usize,           // Time step counter
    pub alpha: f64,         // Learning rate (0.001)
    pub beta1: f64,         // First moment decay (0.9)
    pub beta2: f64,         // Second moment decay (0.999)
    pub epsilon: f64,       // Numerical stability (1e-8)
}
```

### Key Methods

1. **`compute_update(gradient_vector)`** - Computes parameter updates using Adam rule
2. **`reset()`** - Resets optimizer state (useful when changing market regimes)
3. **`get_effective_learning_rate(param_index)`** - Returns adaptive learning rate for a parameter

### Integration with Background Optimization

Every 60 seconds, the background task:
1. Runs grid search to find optimal control
2. Calculates gradients via finite differences (numerical differentiation)
3. Passes gradients to Adam optimizer
4. Adam computes adaptive parameter updates
5. Updates are applied and saved to `tuning_params.json`
6. **Bot continues running with updated parameters** (no reload needed)

### Adam as the Sole Authority

The bot operates in a **closed-loop autonomous mode**:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Human Operator                              ‚îÇ
‚îÇ  (only at startup/restart)                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ
              ‚Üì (loads once)
     tuning_params.json
              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Market Maker Bot                            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ  Adam Optimizer (autonomous)        ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Monitors performance             ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Calculates gradients             ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Updates parameters every 60s     ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Saves to JSON (persistence)      ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚Üë
              ‚îÇ (market data)
         Trading Venue
```

**Key Point**: Once the bot is running, Adam is in full control. There are no external interruptions, file checks, or hot-reloads. This ensures:
- Smooth, uninterrupted learning trajectory
- No race conditions between file I/O and optimization
- Predictable, monotonic improvement over time

## Monitoring

The system logs detailed information for each optimization step:

```
Control gap detected: 7.250000 bps¬≤ (bid_gap=2.50bps, ask_gap=1.00bps). Running Adam optimizer parameter tuning...
Gradient[skew_adjustment_factor] = 0.012345
Gradient[adverse_selection_adjustment_factor] = -0.003456
...
Adam Optimizer State:
  Time step: 42
  Base learning rate (Œ±): 0.001
  Effective LR[skew_adjustment_factor]: 0.000234
  Effective LR[adverse_selection_adjustment_factor]: 0.000891
  ...
Adam Update Applied: TuningParams { ... }
Parameter Changes:
  skew_adjustment_factor: 0.500000 -> 0.497654 (Œî=-0.002346)
  ...
```

### Understanding the Logs

**Control Gap**:
```
Control gap detected: 7.250000 bps¬≤
```
- This is the sum of squared differences in quote offsets
- `‚àö(7.25/2) ‚âà 1.9 bps` RMS difference per quote
- Threshold is 1.0 bps¬≤ (about 0.7 bps RMS per quote)

## Tuning the Optimizer (Advanced)

While the defaults work well, you can customize Adam's hyperparameters:

### Base Learning Rate (`Œ±`)
- **Default**: `0.001` (conservative)
- **Aggressive**: `0.01` - Faster adaptation, more volatile
- **Conservative**: `0.0001` - Slower adaptation, more stable
- **Rule of thumb**: Start with default, only change if convergence issues

### Momentum (`Œ≤‚ÇÅ`)
- **Default**: `0.9` (standard)
- **High momentum**: `0.95` - Smoother, slower to change direction
- **Low momentum**: `0.8` - More responsive to recent gradients
- **Use case**: Increase in choppy markets, decrease in trending markets

### Variance Decay (`Œ≤‚ÇÇ`)
- **Default**: `0.999` (standard)
- **Rarely needs adjustment**
- **Lower values** (`0.99`) can help if gradients are very noisy

### When to Reset

The optimizer automatically resets when:
- Invalid parameters are produced (violate constraints)
- You manually call `optimizer.reset()`

Consider manual reset when:
- Switching trading assets
- Major market regime change
- After extended downtime

## Performance Characteristics

### Computational Cost
- **Gradient calculation**: ~7 evaluations per optimization cycle
- **Adam update**: O(n) where n=7 parameters (negligible)
- **Total**: ~200-500ms per cycle (background thread, non-blocking)

### Memory Overhead
- **Optimizer state**: 2 √ó 7 √ó 8 bytes = 112 bytes (negligible)
- **No significant memory impact**

### Convergence
- **Typical convergence**: 10-50 optimization cycles
- **Time to optimal**: 10-50 minutes (at 60s intervals)
- **Ongoing adaptation**: Continues to track market regime changes

## Comparison with Vanilla SGD

| Metric | Vanilla SGD | Adam Optimizer |
|--------|-------------|----------------|
| Manual tuning required | High | Low |
| Convergence speed | Slow | Fast (2-5x) |
| Stability | Sensitive to LR | Very stable |
| Noise handling | Poor | Excellent |
| Regime adaptation | Limited | Automatic |
| Parameter-specific LR | No | Yes |
| Recommended for production | No | Yes |

## References

- Kingma, D. P., & Ba, J. (2015). Adam: A Method for Stochastic Optimization. ICLR 2015.
- Ruder, S. (2016). An overview of gradient descent optimization algorithms. arXiv:1609.04747

## Next Steps

The Adam optimizer provides a strong foundation for automatic parameter tuning. Future enhancements could include:

1. **Adaptive threshold**: Dynamically adjust the 1.0 bps¬≤ control gap threshold
2. **Gradient clipping**: Prevent extreme updates during market shocks
3. **Parameter-specific constraints**: Different bounds for different market conditions
4. **Multi-objective optimization**: Balance P&L vs. inventory vs. risk simultaneously
5. **Online learning**: Update more frequently (every 10-30s instead of 60s)
